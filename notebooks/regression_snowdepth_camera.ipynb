{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corrected-intelligence",
   "metadata": {},
   "source": [
    "# Regression model for snow depth in time lapse images\n",
    "\n",
    "SnowEx Hackweek 2021 \n",
    "\n",
    "*#cam_learning*\n",
    "\n",
    "__Contributors:__ Marianne Cowherd, Danny Hogan, Katie Breen, Ching-ping Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-frederick",
   "metadata": {},
   "source": [
    "### __Objectives:__\n",
    "\n",
    "- Train a regression model for extracting snow depth from time-lapse imagery using supervised learning\n",
    "- Evaluate model for accuracy \n",
    "- Test potential improvemnts (i.e. cropping images) and suggest ideas for next steps\n",
    "- Learn ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-virtue",
   "metadata": {},
   "source": [
    "### __Motivations:__\n",
    "\n",
    "- 2020 SnowEx time-lapse imagery was labeled for snow-depth, but the process was time-consuming. \n",
    "- Automated methods exist using color thresholding and the Hough Transform, but background pixels add uncertainty.\n",
    "- Computer vision may be able to detect the pole without including the background noise, and identify the snow depth information \n",
    "- 2017 SnowEx time-lapse has not been labeled, and a working ML model could be applied on these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dried-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert moving video of all the camera images and snow depths underneath (Katie will add for funsies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-david",
   "metadata": {},
   "source": [
    "### __Methods__\n",
    "\n",
    "We will use the 2020 SnowEx timelapse from one camera (W1A) as our predictor and the corresponding snow depth measurements in the SnowEx SQL database as the response, to build a supervised model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-aluminum",
   "metadata": {},
   "source": [
    "__1) Load packages for image and data table pre-processing, model development, and model evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "returning-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this part of the tutorial uses additional libraries not in the default snowex jupyterhub\n",
    "# mamba is a python package management alternative to conda and pip https://github.com/mamba-org/mamba\n",
    "!mamba install -y -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eastern-brazilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.3.56-cp38-cp38-manylinux2014_x86_64.whl (37.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.1 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from opencv-python-headless) (1.21.0)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.5.3.56\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "special-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Load packages for machine learning\n",
    "import tensorflow as tf  # end-to-end open source platform for machine learning\n",
    "\n",
    "# from tensorflow.keras.datasets import cifar10\n",
    "# keras is python and uses tensorflow in the backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "#### Packages for image processing and computer vision \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import geopandas as gpd\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "### Packages for processing snow depth values using the SnowEx SQL database \n",
    "from snowexsql.db import get_table_attributes\n",
    "\n",
    "import snowexsql.db\n",
    "from snowexsql.data import PointData, SiteData\n",
    "from snowexsql.conversions import query_to_geopandas\n",
    "# Import the function to get connect to the db\n",
    "from snowexsql.db import get_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-leisure",
   "metadata": {},
   "source": [
    "__2. After loading the packages, we will load in the images from the Amazon Web Services S3. We will focus on one camera, W1A.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "thorough-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the images \n",
    "files = os.listdir('/tmp/camera-trap/W1A')\n",
    "files =  ['/tmp/camera-trap/W1A/' + str(f) for f in files]\n",
    "\n",
    "df = pd.DataFrame([],\n",
    "                   columns=['date','photo_id','time','datetime','depth'])\n",
    "\n",
    "pixels = np.array(pixels)\n",
    "\n",
    "\n",
    "for i in range(0,len(files)): \n",
    "    \n",
    "    img = Image.open(files[i])\n",
    "    exif = { ExifTags.TAGS[k]: v for k, v in img._getexif().items() if k in ExifTags.TAGS }\n",
    "    exif['DateTime'] = datetime.strptime(exif['DateTime'],'%Y:%m:%d %H:%M:%S')\n",
    "    df.loc[i]= [exif['DateTime'].date(),\n",
    "                       files[i],\n",
    "                       exif['DateTime'].time(),exif['DateTime'],np.nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-azerbaijan",
   "metadata": {},
   "source": [
    "__3. Turn the list of images into a list of arrays, and then put the list in one dataframe ('pixels')__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    pivot = df['datetime'][i]\n",
    "    items = df_veg['datetime']\n",
    "    tmp = np.where(items==pivot)[0]\n",
    "    if len(np.where(items==pivot)[0])>0:\n",
    "        idx = tmp[0]\n",
    "        df['depth'][i] = df_veg['value'][idx]\n",
    "\n",
    "pixels = []      \n",
    "for i in range(0, len(df)):\n",
    "    # img = cv2.imread(str(path)+\"/\"+str(img))\n",
    "    # src = Image.open(str(path)+\"/\"+str(img))\n",
    "    path = df['photo_id'][i]\n",
    "    src = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    #calculate the 50 percent of original dimensions\n",
    "    width =200 # int(src.shape[1] * scale_percent / 100)\n",
    "    height = 200 # int(src.shape[0] * scale_percent / 100)\n",
    "    # dsize\n",
    "    dsize = (width, height)\n",
    "    # resize image\n",
    "    output = cv2.resize(src, dsize)\n",
    "    cv2.imwrite('tmp.jpg',output) \n",
    "    # img1 = img.save('tmp', format='JPEG',dpi=(50,50))\n",
    "    img2 = cv2.imread('tmp.jpg')\n",
    "    img2 = cv2.cvtColor(img2,cv2.COLOR_BGR2RGB)\n",
    "    pixels.append(np.array(img2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-relation",
   "metadata": {},
   "source": [
    "Note: This is a data table with XX number of rows (i.e. number of images) and XX columns! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-folks",
   "metadata": {},
   "source": [
    "__4. Pull the snow depth values from the SnowEx SQL database__\n",
    "\n",
    "In this case, we pulled all the data from the snow depth data from camera W1A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to see what columns are available to use. \n",
    "db_columns = get_table_attributes(PointData)\n",
    "\n",
    "# Print out the results nicely\n",
    "# print(\"These are the available columns in the table:\\n \\n* {}\\n\".format('\\n* '.join(db_columns)))\n",
    "\n",
    "# Grab the open site data from the db\n",
    "open_site = 'W1A'\n",
    "qry = session.query(PointData).filter(PointData.equipment.contains(open_site))\n",
    "df_open = query_to_geopandas(qry,engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-pharmaceutical",
   "metadata": {},
   "source": [
    "We will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([],\n",
    "                   columns=['date','photo_id','time','datetime','depth'])\n",
    "for i in range(0,len(files)): \n",
    "    \n",
    "    img = Image.open(files[i])\n",
    "    exif = { ExifTags.TAGS[k]: v for k, v in img._getexif().items() if k in ExifTags.TAGS }\n",
    "    exif['DateTime'] = datetime.strptime(exif['DateTime'],'%Y:%m:%d %H:%M:%S')\n",
    "    df.loc[i]= [exif['DateTime'].date(),\n",
    "                       files[i],\n",
    "                       exif['DateTime'].time(),exif['DateTime'],np.nan]\n",
    "    \n",
    "df_open['datetime'] = [datetime.combine(df_open['date'][i],df_open['time'][i]).replace(tzinfo=None) for i in range(len(df_open))]\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    pivot = df['datetime'][i]\n",
    "    items = df_open['datetime']\n",
    "    tmp = np.where(items==pivot)[0]\n",
    "    if len(np.where(items==pivot)[0])>0:\n",
    "        idx = tmp[0]\n",
    "        df['depth'][i] = df_open['value'][idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
