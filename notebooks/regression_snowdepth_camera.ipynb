{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "express-bracket",
   "metadata": {},
   "source": [
    "# Regression model for snow depth in time lapse images\n",
    "\n",
    "SnowEx Hackweek 2021 \n",
    "\n",
    "*#cam_learning*\n",
    "\n",
    "__Contributors:__ Marianne Cowherd, Danny Hogan, Katie Breen, Ching-ping Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-terror",
   "metadata": {},
   "source": [
    "### __Objectives:__\n",
    "\n",
    "- Train a regression model for extracting snow depth from time-lapse imagery using supervised learning\n",
    "- Evaluate model for accuracy \n",
    "- Test potential improvemnts (i.e. cropping images) and suggest ideas for next steps\n",
    "- Learn ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-olympus",
   "metadata": {},
   "source": [
    "### __Motivations:__\n",
    "\n",
    "- 2020 SnowEx time-lapse imagery was labeled for snow-depth, but the process was time-consuming. \n",
    "- Automated methods exist using color thresholding and the Hough Transform, but background pixels add uncertainty.\n",
    "- Computer vision may be able to detect the pole without including the background noise, and identify the snow depth information \n",
    "- 2017 SnowEx time-lapse has not been labeled, and a working ML model could be applied on these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rotary-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert moving video of all the camera images and snow depths underneath (Katie will add once she gets the code from Cassie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-adobe",
   "metadata": {},
   "source": [
    "### __Methods__\n",
    "\n",
    "We will use the 2020 SnowEx timelapse from one camera (W1A) as our predictor and the corresponding snow depth measurements in the SnowEx SQL database as the response, to build a supervised model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-planet",
   "metadata": {},
   "source": [
    "__1) Load packages for image and data table pre-processing, model development, and model evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "guided-wholesale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in /srv/conda/envs/notebook/lib/python3.8/site-packages (4.5.3.56)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from opencv-python-headless) (1.21.0)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this part of the tutorial uses additional libraries not in the default snowex jupyterhub\n",
    "# mamba is a python package management alternative to conda and pip https://github.com/mamba-org/mamba\n",
    "!mamba install -y -q tensorflow\n",
    "!pip install opencv-python-headless "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "economic-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Load packages for machine learning\n",
    "import tensorflow as tf  # end-to-end open source platform for machine learning\n",
    "\n",
    "# from tensorflow.keras.datasets import cifar10\n",
    "# keras is python and uses tensorflow in the backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "#### Packages for image processing and computer vision \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import geopandas as gpd\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "### Packages for processing snow depth values using the SnowEx SQL database \n",
    "from snowexsql.db import get_table_attributes\n",
    "\n",
    "import snowexsql.db\n",
    "from snowexsql.data import PointData, SiteData\n",
    "from snowexsql.conversions import query_to_geopandas\n",
    "# Import the function to get connect to the db\n",
    "from snowexsql.db import get_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-gnome",
   "metadata": {},
   "source": [
    "__2. After loading the packages, we will load in the images from the Amazon Web Services S3. We will focus on one camera, W1A.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "occupational-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the images \n",
    "files = os.listdir('/tmp/camera-trap/W1A')\n",
    "files =  ['/tmp/camera-trap/W1A/' + str(f) for f in files]\n",
    "\n",
    "df = pd.DataFrame([],\n",
    "                   columns=['date','photo_id','time','datetime','depth'])\n",
    "\n",
    "for i in range(0,len(files)): \n",
    "    \n",
    "    img = Image.open(files[i])\n",
    "    exif = { ExifTags.TAGS[k]: v for k, v in img._getexif().items() if k in ExifTags.TAGS }\n",
    "    exif['DateTime'] = datetime.strptime(exif['DateTime'],'%Y:%m:%d %H:%M:%S')\n",
    "    df.loc[i]= [exif['DateTime'].date(),\n",
    "                       files[i],\n",
    "                       exif['DateTime'].time(),exif['DateTime'],np.nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-importance",
   "metadata": {},
   "source": [
    "__3. Read in the images using cv2's imread function, then downscale the images to 200 x 200 pixels. 'pixels' is a list of all the images represented as an RGB array.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "alike-hunger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "pixels = []      \n",
    "for i in range(0, len(df)):\n",
    "    # img = cv2.imread(str(path)+\"/\"+str(img))\n",
    "    # src = Image.open(str(path)+\"/\"+str(img))\n",
    "    path = df['photo_id'][i]\n",
    "    src = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    #calculate the 50 percent of original dimensions\n",
    "    width =200 # int(src.shape[1] * scale_percent / 100)\n",
    "    height = 200 # int(src.shape[0] * scale_percent / 100)\n",
    "    # dsize\n",
    "    dsize = (width, height)\n",
    "    # resize image\n",
    "    output = cv2.resize(src, dsize)\n",
    "    cv2.imwrite('tmp.jpg',output) \n",
    "    # img1 = img.save('tmp', format='JPEG',dpi=(50,50))\n",
    "    img2 = cv2.imread('tmp.jpg')\n",
    "    img2 = cv2.cvtColor(img2,cv2.COLOR_BGR2RGB)\n",
    "    pixels.append(np.array(img2))\n",
    "\n",
    "pixels = np.array(pixels)\n",
    "print(pixels.shape) \n",
    "#print(pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-participation",
   "metadata": {},
   "source": [
    "__4. We will not flatten the images into a dataframe with one row for each image and columns for all the RGB pixel values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alternative-motor",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pixels_open.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6ca2bf11ee68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#np.save('pixels_open.npy',pixels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpixels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pixels_open.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pixels_open.npy'"
     ]
    }
   ],
   "source": [
    "dataset = pixels.reshape((659,-1))\n",
    "dataset = np.concatenate((dataset, np.array(df['depth']).reshape((659,1))),axis=1)\n",
    "dataset=pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "alleged-means",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 120000 into shape (659,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-347adf461071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m659\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'depth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m659\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 120000 into shape (659,newaxis)"
     ]
    }
   ],
   "source": [
    "dataset = pixels.reshape((659,-1))\n",
    "dataset = np.concatenate((dataset, np.array(df['depth']).reshape((659,1))),axis=1)\n",
    "dataset=pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-albania",
   "metadata": {},
   "source": [
    "Note: This is a data table with XX number of rows (i.e. number of images) and XX columns! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-liabilities",
   "metadata": {},
   "source": [
    "__4. Pull the snow depth values from the SnowEx SQL database__\n",
    "\n",
    "In this case, we pulled all the data from the snow depth data from camera W1A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to see what columns are available to use. \n",
    "db_columns = get_table_attributes(PointData)\n",
    "\n",
    "# Print out the results nicely\n",
    "# print(\"These are the available columns in the table:\\n \\n* {}\\n\".format('\\n* '.join(db_columns)))\n",
    "\n",
    "# Grab the open site data from the db\n",
    "open_site = 'W1A'\n",
    "qry = session.query(PointData).filter(PointData.equipment.contains(open_site))\n",
    "df_open = query_to_geopandas(qry,engine)\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    pivot = df['datetime'][i]\n",
    "    items = df_veg['datetime']\n",
    "    tmp = np.where(items==pivot)[0]\n",
    "    if len(np.where(items==pivot)[0])>0:\n",
    "        idx = tmp[0]\n",
    "        df['depth'][i] = df_veg['value'][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-mandate",
   "metadata": {},
   "source": [
    "We will "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-grass",
   "metadata": {},
   "source": [
    "__5. We will do the same as before but this time with cropped images. We will compare our results between the full images and the cropped images.__\n",
    "\n",
    "We will collapse the code to save space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "binary-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped =  ['/data/cropped/W1A/' for f in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "secure-armstrong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/cropped/W1A/']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-authentication",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
